{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The k-NN algorithm (recommendation system)\n",
    "\n",
    "Let's create a recommendation system based on k-NN, where it can predict your rating for something. This will be roughly analogous to [Collaborative Filtering](https://en.wikipedia.org/wiki/Collaborative_filtering).\n",
    "\n",
    "We will create vectors based on ratings. This is likely to be highly inefficient!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data\n",
    "\n",
    "[MovieLens](https://grouplens.org/) provides datasets from the data they use for their own recommendation system. As they don't provide any archived/fixed-in-time data file, we'll obtain one from <https://archive.org> (2016/11/16). The download is likely to be very slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Data\n",
    "import os\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "\n",
    "# Source: https://grouplens.org/datasets/movielens/latest/\n",
    "# Size: 230MB\n",
    "DATA_PATH = Path('data/2-movielens.zip')\n",
    "DATA_URL = 'https://web.archive.org/web/20161112181144/http://files.grouplens.org/datasets/movielens/ml-latest.zip'\n",
    "if not DATA_PATH.exists():\n",
    "    Path('data').mkdir(exist_ok=True)\n",
    "    urllib.request.urlretrieve(DATA_URL, DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data will need significant processing before we can use it for recommendations.\n",
    "\n",
    "The first and most important file is the `ratings.csv` file. This contains timestamped user ratings for movies on a scale of 0.5-5.0 stars (0.5 increments). This is sparse, but k-NN cannot handle sparse data so we'll look at how we can get around this.\n",
    "\n",
    "Due to the data sizes involved, we're going to start using Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 24404096 entries, 0 to 24404095\n",
      "Data columns (total 4 columns):\n",
      "userId       int32\n",
      "movieId      int32\n",
      "rating       int8\n",
      "timestamp    datetime64[ns]\n",
      "dtypes: datetime64[ns](1), int32(2), int8(1)\n",
      "memory usage: 395.7 MB\n"
     ]
    }
   ],
   "source": [
    "# Read data\n",
    "# CSV structure: userId, movieId, rating, timestamp.\n",
    "import contextlib\n",
    "import zipfile\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "with zipfile.ZipFile(DATA_PATH) as zipin:\n",
    "    zipin.testzip()\n",
    "    with zipin.open('ml-latest/ratings.csv') as fin:\n",
    "        data_df = pd.read_csv(\n",
    "            fin,\n",
    "            dtype={'userId': np.int32, 'movieId': np.int32, 'timestamp': np.int32},\n",
    "            converters={'rating': lambda x: np.int8(float(x) * 2)},\n",
    "            parse_dates=['timestamp'],\n",
    "            date_parser=datetime.utcfromtimestamp,\n",
    "        )\n",
    "\n",
    "# Doesn't seem to be possible to set dtype when using converter\n",
    "data_df['rating'] = data_df['rating'].astype(np.int8)\n",
    "data_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's exclude anyone with less than 100 ratings, and also exclude any movie with less than 1000 ratings.\n",
    "\n",
    "It feels as though there is some point where we don't have enough information, but these are arbitrary cut-offs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering by movies...\n",
      "Cutting down to 3577 movies from 39443 original movies\n",
      "Cutting down to 21883270 ratings from 24404096 previous #ratings\n",
      "Filtering by users...\n",
      "Cutting down to 57659 users from 258802 original users\n",
      "Cutting down to 16175345 ratings from 21883270 previous #ratings\n",
      "Filtering by movies...\n",
      "Cutting down to 3296 movies from 3577 original movies\n",
      "Cutting down to 15919146 ratings from 16175345 previous #ratings\n",
      "Filtering by users...\n",
      "Cutting down to 57091 users from 57659 original users\n",
      "Cutting down to 15863428 ratings from 15919146 previous #ratings\n",
      "Filtering by movies...\n",
      "Cutting down to 3287 movies from 3296 original movies\n",
      "Cutting down to 15854464 ratings from 15863428 previous #ratings\n",
      "Filtering by users...\n",
      "Cutting down to 57076 users from 57091 original users\n",
      "Cutting down to 15852979 ratings from 15854464 previous #ratings\n",
      "Filtering by movies...\n",
      "Cutting down to 3287 movies from 3287 original movies\n",
      "Cutting down to 15852979 ratings from 15852979 previous #ratings\n",
      "Filtering by users...\n",
      "Cutting down to 57076 users from 57076 original users\n",
      "Cutting down to 15852979 ratings from 15852979 previous #ratings\n"
     ]
    }
   ],
   "source": [
    "filtered_df_0 = data_df\n",
    "\n",
    "while True:\n",
    "    print('Filtering by movies...')\n",
    "    movie_rating_counts = filtered_df_0.groupby('movieId', sort=False)['rating'].count()\n",
    "    movies_with_sufficient_ratings = movie_rating_counts[movie_rating_counts >= 1000]\n",
    "    filtered_df_1 = filtered_df_0[filtered_df_0['movieId'].isin(movies_with_sufficient_ratings.index)]\n",
    "    print('Cutting down to', len(movies_with_sufficient_ratings), 'movies from', len(movie_rating_counts), 'original movies')\n",
    "    print('Cutting down to', len(filtered_df_1), 'ratings from', len(filtered_df_0), 'previous #ratings')\n",
    "\n",
    "    print('Filtering by users...')\n",
    "    user_rating_counts = filtered_df_1.groupby('userId', sort=False)['rating'].count()\n",
    "    users_with_sufficient_ratings = user_rating_counts[user_rating_counts >= 100]\n",
    "    filtered_df_2 = filtered_df_1[filtered_df_1['userId'].isin(users_with_sufficient_ratings.index)]\n",
    "    print('Cutting down to', len(users_with_sufficient_ratings), 'users from', len(user_rating_counts), 'original users')\n",
    "    print('Cutting down to', len(filtered_df_2), 'ratings from', len(filtered_df_1), 'previous #ratings')\n",
    "    \n",
    "    if len(filtered_df_2) == len(filtered_df_0):\n",
    "        break\n",
    "    filtered_df_0 = filtered_df_2\n",
    "\n",
    "filtered_df = filtered_df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10      userId  movieId  rating           timestamp\n",
      "232      10        1       8 2005-02-04 18:27:46\n",
      "233      10        2       6 2005-02-04 18:37:36\n",
      "234      10        3       7 2005-02-04 18:41:40\n",
      "235      10        6       8 2005-02-04 18:33:49\n",
      "236      10       10       6 2005-02-04 18:30:45\n",
      "237      10       11       8 2005-02-04 18:35:57\n",
      "238      10       16       7 2005-02-04 18:43:39\n",
      "239      10       21       8 2005-02-04 18:32:29\n",
      "240      10       22       6 2005-02-04 18:00:34\n",
      "241      10       32       5 2005-02-04 18:28:13\n",
      "242      10       34       7 2005-02-04 18:30:06\n",
      "243      10       50       6 2005-02-04 18:28:57\n",
      "244      10       62       8 2005-02-04 18:33:55\n",
      "245      10      105       7 2005-02-04 18:01:50\n",
      "246      10      110       8 2005-02-04 18:26:58\n",
      "247      10      111       5 2005-02-04 18:35:43\n",
      "248      10      141       5 2005-02-04 18:32:51\n",
      "249      10      150       6 2005-02-04 18:26:53\n",
      "250      10      160       5 2005-02-04 18:40:13\n",
      "251      10      161       8 2005-02-04 18:31:35\n",
      "252      10      165       8 2005-02-04 18:28:45\n",
      "253      10      185       8 2005-02-04 18:32:15\n",
      "254      10      208       5 2005-02-04 18:32:55\n",
      "255      10      225       8 2005-02-04 18:42:28\n",
      "256      10      235       6 2005-02-04 18:41:25\n",
      "257      10      253       3 2005-02-04 18:32:25\n",
      "258      10      292       6 2005-02-04 18:30:52\n",
      "259      10      296       6 2005-02-04 18:15:34\n",
      "260      10      316       5 2005-02-04 18:29:02\n",
      "261      10      317       7 2005-02-04 18:38:31\n",
      "..      ...      ...     ...                 ...\n",
      "395      10     2826       5 2005-02-04 18:08:12\n",
      "396      10     2890       6 2005-02-04 18:15:44\n",
      "397      10     2916       7 2005-02-04 18:15:49\n",
      "398      10     2918       7 2005-02-04 18:44:18\n",
      "399      10     2959       6 2005-02-04 18:41:12\n",
      "400      10     2997       7 2005-02-04 18:37:30\n",
      "401      10     3114       8 2005-02-04 18:14:51\n",
      "402      10     3176       6 2005-02-04 18:01:21\n",
      "403      10     3258       4 2005-02-04 18:13:34\n",
      "404      10     3363       6 2005-02-04 18:06:57\n",
      "405      10     3471       7 2005-02-04 18:00:21\n",
      "406      10     3578      10 2005-02-04 18:38:08\n",
      "407      10     3593       6 2005-02-04 18:06:46\n",
      "408      10     3751       8 2005-02-04 18:15:14\n",
      "409      10     3753       7 2005-02-04 18:14:15\n",
      "410      10     3996       9 2005-02-04 18:05:07\n",
      "411      10     4025       7 2005-02-04 18:06:40\n",
      "412      10     4027       9 2005-02-04 18:00:49\n",
      "413      10     4085       6 2005-02-04 18:06:52\n",
      "414      10     4638       7 2005-02-04 18:13:42\n",
      "415      10     4995       6 2005-02-04 18:07:05\n",
      "416      10     5093       8 2005-02-04 18:11:31\n",
      "417      10     5171       6 2005-02-04 18:11:25\n",
      "418      10     5312       6 2005-02-04 18:10:47\n",
      "419      10     5464       6 2005-02-04 18:07:14\n",
      "420      10     5943       6 2005-02-04 18:10:15\n",
      "421      10     5956       8 2005-02-04 18:05:43\n",
      "422      10     6662       8 2005-02-04 18:12:02\n",
      "423      10     7090       7 2005-02-04 18:11:18\n",
      "424      10     7143       7 2005-02-04 18:06:02\n",
      "\n",
      "[193 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "filtered_df.head(2)\n",
    "for g, d in filtered_df.groupby('userId'):\n",
    "    print(g, d)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-NN methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distance(user_1_df, user_2_df, threshold=20):\n",
    "    \"\"\"Return the distance between two users.\n",
    "    \n",
    "    If we don't have threshold shared ratings for movies, then we return None.\n",
    "    \"\"\"\n",
    "    shared_ratings = pd.merge(user_1_df, user_2_df, how='inner', on=['movieId'])\n",
    "    if len(shared_ratings) < threshold:\n",
    "        return None\n",
    "    \n",
    "    return sum((shared_ratings['rating_x'] - shared_ratings['rating_y'])**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "from operator import itemgetter\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def get_movie_prediction(grouped_users_df, test_user_df, movie_id, knn=5, distance_func=get_distance, threshold=20):\n",
    "    \"\"\"Return our prediction for the particular movie.\"\"\"\n",
    "    # We should remove the movie from test_user_df!\n",
    "    test_user_df = test_user_df[test_user_df['movieId'] != movie_id]\n",
    "\n",
    "    # Build up a list of distances to users\n",
    "    print(datetime.utcnow())\n",
    "    user_distances = []  # (user, distance)\n",
    "    for user_id, train_user_df in grouped_users_df:\n",
    "        distance = distance_func(train_user_df, test_user_df, threshold=threshold)\n",
    "        # Skip users with insufficient data for a good distance\n",
    "        if distance:\n",
    "            user_distances.append((user_id, distance))\n",
    "    user_distances.sort(key=itemgetter(1))\n",
    "    neighbours = map(itemgetter(0), user_distances)\n",
    "    \n",
    "    # Average the ratings of the knn closest users\n",
    "    print(datetime.utcnow())\n",
    "    user_movie_ratings = []\n",
    "    for next_closest_user_id in neighbours:\n",
    "        user_movie_rating = grouped_users_df.get_group(next_closest_user_id).query('movieId==' + str(movie_id))\n",
    "        try:\n",
    "            user_movie_ratings.append(float(user_movie_rating.at[user_movie_rating.index[0], 'rating']))\n",
    "        except IndexError:\n",
    "            continue\n",
    "\n",
    "        if len(user_movie_ratings) >= knn:\n",
    "            break\n",
    "\n",
    "    print(user_movie_ratings)\n",
    "    print(datetime.utcnow())\n",
    "    prediction = statistics.mean(user_movie_ratings)\n",
    "    return prediction\n",
    "\n",
    "\n",
    "def get_prediction(grouped_users_df, test_user_df, movies, knn=5, distance_func=get_distance, threshold=20):\n",
    "    \"\"\"Return our predictions for for the test user for all movies.\"\"\"\n",
    "    test_user_id = test_user_df.iloc[0].loc['userId']\n",
    "    predictions = []\n",
    "    for movie_id in movies:\n",
    "        prediction = get_movie_prediction(\n",
    "            grouped_users_df,\n",
    "            test_user_df,\n",
    "            movie_id,\n",
    "            knn=knn,\n",
    "            distance_func=distance_func,\n",
    "            threshold=threshold,\n",
    "        )\n",
    "        predictions.append({\n",
    "            'userId': test_user_id,\n",
    "            'movieId': movie_id,\n",
    "            'prediction': prediction,\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test some code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_users_df = filtered_df.groupby('userId')\n",
    "test_user = filtered_df[filtered_df['userId'] == 14]\n",
    "movies = filtered_df['movieId'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-03-02 17:14:12.633213\n",
      "2018-03-02 17:16:20.961153\n",
      "[9.0, 9.0, 8.0, 9.0, 6.0]\n",
      "2018-03-02 17:16:20.981072\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8.2"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_movie_prediction(grouped_users_df, test_user, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(grouped_users_df, test_user_df, movies, knn=5, distance_func=get_distance, threshold=25):\n",
    "    \"\"\"Return an %-accuracy table perfect, off-by-1, off-by-2.\"\"\"\n",
    "    correct_count = 0\n",
    "    for test_datum in test_data:\n",
    "        prediction = get_prediction(train_data, test_datum, knn=knn)\n",
    "        if prediction == test_datum[-1]:\n",
    "            correct_count += 1\n",
    "    \n",
    "    return (correct_count / len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_user_df = test_user[test_user['movieId'] != 1]\n",
    "\n",
    "\n",
    "#     shared_ratings = pd.merge(user_1_df, user_2_df, how='inner', on=['movieId'])\n",
    "#     if len(shared_ratings) < threshold:\n",
    "#         return None\n",
    "\n",
    "def aa(group):\n",
    "    return get_distance(group, test_user_df, threshold=20)\n",
    "\n",
    "def ff(group):\n",
    "    return group['movieId'].isin(test_user_df['movieId']).count() > 20\n",
    "\n",
    "xx = grouped_users_df.filter(ff).any().apply(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
