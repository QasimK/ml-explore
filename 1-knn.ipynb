{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The k-NN Algorithm\n",
    "The k-nearest neighbours classifier/regressor is one of the simplest machine learning algorithms.\n",
    "\n",
    "For example, when the algorithm wants to classify a new instance, it will look at all of the data it has and picks the 5 closest instances and use that to determine the class. This means there is no real training phase - it always uses all of the \"training\" data for each input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data\n",
    "\n",
    "We will take a look at a famous small dataset of three distinct species of Iris flowers:\n",
    "\n",
    "| Iris Setosa | Iris Veriscolor | Iris Virginica |\n",
    "|-------------|-----------------|----------------|\n",
    "| <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/5/56/Kosaciec_szczecinkowaty_Iris_setosa.jpg/450px-Kosaciec_szczecinkowaty_Iris_setosa.jpg\" /> | <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/2/27/Blue_Flag%2C_Ottawa.jpg/480px-Blue_Flag%2C_Ottawa.jpg\" /> | <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/9/9f/Iris_virginica.jpg/590px-Iris_virginica.jpg\"/> |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Data\n",
    "import os\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "\n",
    "# Source: https://archive.ics.uci.edu/ml/datasets/iris\n",
    "DATA_PATH = Path('data/1-iris.csv')\n",
    "DATA_URL = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\n",
    "if not DATA_PATH.exists():\n",
    "    Path('data').mkdir(exist_ok=True)\n",
    "    urllib.request.urlretrieve(DATA_URL, DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Data\n",
    "# Data Structure: SepalLengthCm, SepalWidthCm, PetalLengthCm, PetalWidthCm, IrisType\n",
    "import csv\n",
    "\n",
    "data = []\n",
    "with open(DATA_PATH) as fin:\n",
    "    for row in csv.reader(fin):\n",
    "        if not row:\n",
    "            continue\n",
    "        data.append((float(row[0]), float(row[1]), float(row[2]), float(row[3]), row[4]))\n",
    "\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the k-nearest neighbour algorithm?\n",
    "\n",
    "Work out the nearest neighbours out by figuring out the distance to all of them, one-by-one:\n",
    "\n",
    "![Distance between points](http://passyworldofmathematics.com/Images/pwmImagesSeven/DistanceBetweenPointsThree550x390JPG.jpg)\n",
    "\n",
    "$$distance = \\sqrt{(y_1 - x_1)^2 + (y_2 + x_2)^2}$$\n",
    "\n",
    "Then take the `k` closest points - these are our k-nearest neighbours!\n",
    "\n",
    "Our prediction will be the most common label in those `k` neighbours:\n",
    "\n",
    "| Distance | Label           |\n",
    "|----------|-----------------|\n",
    "|   1.1    | Iris-setosa     |\n",
    "|   1.5    | Iris-setosa     |\n",
    "|   2.2    | Iris-versicolor |\n",
    "|   2.3    | Iris-setosa     |\n",
    "|   2.9    | Iris-setosa     |\n",
    "\n",
    "In this case `Iris-setosa` wins by 4/5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def get_distance(data_1, data_2):\n",
    "    \"\"\"Return the distance between two points.\"\"\"\n",
    "    return sum((x2 - x1)**2 for x1, x2 in zip(data_1[:-1], data_2[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from collections import Counter\n",
    "\n",
    "def get_prediction(train_data, test_datum, knn=5, distance_func=get_distance):\n",
    "    \"\"\"Return our prediction for test_datum.\"\"\"\n",
    "    train_label_distances = []\n",
    "    for train_datum in train_data:\n",
    "        label = train_datum[-1]\n",
    "        distance = distance_func(train_datum, test_datum)\n",
    "        train_label_distances.append((label, distance))\n",
    "    \n",
    "    # Sort by distance (smallest to largest)\n",
    "    train_label_distances.sort(key=itemgetter(1))\n",
    "    # Take top k instances\n",
    "    neighbours = train_label_distances[:knn]\n",
    "    neighbouring_labels = [label for label, distance in neighbours]\n",
    "    # Count the labels\n",
    "    counted_neighbours = Counter(neighbouring_labels)\n",
    "    prediction = counted_neighbours.most_common(1)[0][0]\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do we test our algorithm?\n",
    "\n",
    "We will test our algorithm against the data that we already have - rather than going out into a forest to start measuring petal lengths.\n",
    "\n",
    "We can split the data up into training data and test data. There's a lot of ways to split it up. To start off with we will just split it up 50-50.\n",
    "\n",
    "If we take a look at our data we can see that it has all the Iris Setosas first, then all the Veriscolors, then all the Virginicas. If we split this up 50-50, we wouldn't have any Virginicas in our training data - so it would be impossible to make a Virginica prediction!\n",
    "\n",
    "**Randomise the data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def get_train_test_data(data):\n",
    "    \"\"\"Return (train_data, test_data).\"\"\"\n",
    "    # Shuffle data for some variation between runs\n",
    "    random.shuffle(data)\n",
    "    split_line = len(data) // 2\n",
    "    return (data[:split_line], data[split_line:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the accuracy of our predictions, we can count the number of correct and incorrect predictions for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(train_data, test_data, knn):\n",
    "    correct_count = 0\n",
    "    for test_datum in test_data:\n",
    "        prediction = get_prediction(train_data, test_datum, knn=knn)\n",
    "        if prediction == test_datum[-1]:\n",
    "            correct_count += 1\n",
    "    \n",
    "    return (correct_count / len(test_data))\n",
    "\n",
    "def get_50_50_accuracy(data, knn=5):\n",
    "    train_data, test_data = get_train_test_data(data)\n",
    "    return get_accuracy(train_data, test_data, knn=5)\n",
    "\n",
    "get_50_50_accuracy(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do we make use of all of our data?\n",
    "If we run this algorithm we get a range of scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = [get_50_50_accuracy(data) for _ in range(500)]\n",
    "def get_bins(alist):\n",
    "    def frange(x, y, jump):\n",
    "        while x < y:\n",
    "            yield x\n",
    "            x += jump\n",
    "    return list(frange(min(alist), max(alist), 0.01))\n",
    "sns.distplot(accuracies, bins=get_bins(accuracies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can improve our estimate on the accuracy by using **k-fold cross-validation**.\n",
    "\n",
    "Normally, cross-validation is used so that the algorithm can train using all of the data, while still having test (validation) data to prevent **over-fitting**.\n",
    "\n",
    "![k-fold cross validation](https://upload.wikimedia.org/wikipedia/commons/1/1c/K-fold_cross_validation_EN.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kfolds(data, kf):\n",
    "    \"\"\"Yield k (train_data, test_data).\"\"\"\n",
    "    # Shuffle data for some variation between k-fold runs\n",
    "    random.shuffle(data)\n",
    "    test_data_size = len(data) // kf\n",
    "    for i in range(kf):\n",
    "        start_test_data = i * test_data_size\n",
    "        end_test_data = start_test_data + test_data_size\n",
    "        train_data_before = data[:start_test_data]\n",
    "        test_data = data[start_test_data:end_test_data]\n",
    "        train_data_after = data[end_test_data:]\n",
    "        train_data = train_data_before + train_data_after\n",
    "        yield (train_data, test_data)\n",
    "\n",
    "def get_kfold_accuracy(data, kf=5, knn=12):\n",
    "    accuracies = []\n",
    "    for train_data, test_data in get_kfolds(data, kf=kf):\n",
    "        accuracies.append(get_accuracy(train_data, test_data, knn=knn))\n",
    "    return sum(accuracies) / len(accuracies)\n",
    "\n",
    "get_kfold_accuracy(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we take a look at the accuracies splitting the data into 2 chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = [get_kfold_accuracy(data, kf=2) for _ in range(500)]\n",
    "sns.distplot(accuracies, bins=get_bins(accuracies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does the number of neighbours affect things?\n",
    "\n",
    "* Data tends to be noisy! People could have mis-measured the flowers, written down the data incorrectly, or entered the data on the computer incorrectly. Increasing the number of neighbours can mean that one flower that is so obviously wrong is ignored.\n",
    "* But if you increase `k` too much, then you are comparing against *all* of the flowers... it stops being nearest neighbour!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "accuracies = []\n",
    "for knn in range(1, len(data)):\n",
    "    accuracies.append(get_kfold_accuracy(data, knn=knn))\n",
    "    \n",
    "# Screw you, tsplot is the only thing that works.\n",
    "sns.tsplot(accuracies, value='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does the number of k-folds affect things?\n",
    "\n",
    "* The more k-folds you have, the more you are averaging, so it should become more stable.\n",
    "* The more k-folds you have, the greater the training data - reducing the chances to validate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "for kf in range(2, len(data)):\n",
    "    accuracies.append(get_kfold_accuracy(data, kf=kf) for i in range(100))\n",
    "\n",
    "# Screw you, tsplot is the only thing that works.\n",
    "sns.tsplot(accuracies, value='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is it just me, or did that take a while to run?\n",
    "\n",
    "I agree with you Q, that was pretty slow! The performance gets worse with more k-folds k=149, 149*1*149 vs k=2, 75*75*2 ( tr-size * te-size * kf )\n",
    "\n",
    "tr-size * te-size * kf\n",
    "(1 - te-size) * (data / kf ) * kf\n",
    "data * (1 - data / kf )\n",
    "data - data*2/kf\n",
    "\n",
    "As kf goes up, this goes down.\n",
    "\n",
    "Let's time-it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit get_kfold_accuracy(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distance function can be optimised.\n",
    "\n",
    "* Don't take a sqrt - only the relative distances matter.\n",
    "* Use octrees/r-trees/k-d trees/etc. to subdivide objects into regions.\n",
    "\n",
    "This can turn `O(n)` for classifying a single instance to something closer to `O(log n)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at the distance function\n",
    "\n",
    "### Higher dimensions\n",
    "Things get tough here.\n",
    "\n",
    "It's pretttyyyy bad when you have lots of features.\n",
    "Here's an example - let's use only Petal Widths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sls = [[datum[2], datum[-1]] for datum in data]\n",
    "get_kfold_accuracy(data_sls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sls_overboard = [[datum[2]] * 20 + [datum[-1]] for datum in data]\n",
    "get_kfold_accuracy(data_sls_overboard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different feature \"lengths\"\n",
    "\n",
    "If the features were measured in mm, cm and km you wouldn't expected the algorithm to do differently - it's the same data after all!\n",
    "\n",
    "But it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_multiplied = [\n",
    "    [datum[0]*10**6, datum[1]*10**1, datum[2]*10**1, datum[3]*10**1, datum[4]]\n",
    "    for datum in data\n",
    "]\n",
    "get_kfold_accuracy(data_multiplied)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "def normalise(data):\n",
    "    new_cols = []\n",
    "    for col_num in range(len(data[0]) -1):\n",
    "        data_column = [datum[col_num] for datum in data]\n",
    "        mean = statistics.mean(data_column)\n",
    "        std = statistics.stdev(data_column)\n",
    "        new_cols.append([(point - mean) / std for point in data_column])\n",
    "    new_cols.append(datum[-1] for datum in data)\n",
    "    return list(zip(*new_cols))\n",
    "\n",
    "data_normalised = normalise(data)\n",
    "get_kfold_accuracy(data_normalised)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
